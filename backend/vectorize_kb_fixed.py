import asyncio
import sys
sys.path.insert(0, '/app')

from app.db.database import AsyncSessionLocal
from app.services.kb_vectorization_service import KnowledgeBaseVectorizationService
from app.services.vector_embedding_service import VectorEmbeddingService
from app.models.models import KnowledgeBaseChunk
from pathlib import Path
import uuid
from datetime import datetime
import hashlib

class DocumentChunker:
    """ç®€å•çš„æ–‡æ¡£åˆ†å—å™¨"""
    def __init__(self, chunk_size=1000, chunk_overlap=100):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
    
    def split_text(self, text):
        """å°†æ–‡æœ¬åˆ†å—"""
        if not text or not text.strip():
            return []
        
        chunks = []
        start = 0
        text_len = len(text)
        
        while start < text_len:
            end = min(start + self.chunk_size, text_len)
            chunk = text[start:end].strip()
            if chunk:  # åªæ·»åŠ éç©ºå—
                chunks.append(chunk)
            start = end - self.chunk_overlap if end < text_len else end
        
        return chunks

async def vectorize_pediatric_asthma():
    """å‘é‡åŒ–å„¿ç«¥æ”¯æ°”ç®¡å“®å–˜æŒ‡å—"""
    async with AsyncSessionLocal() as db:
        service = KnowledgeBaseVectorizationService(db)
        vector_service = VectorEmbeddingService(db)
        chunker = DocumentChunker(chunk_size=800, chunk_overlap=100)
        
        # æ–‡æ¡£ç›®å½•
        doc_dir = Path("/app/data/knowledge_bases/diseases/pediatric_bronchial_asthma")
        
        if not doc_dir.exists():
            print("âŒ æ–‡æ¡£ç›®å½•ä¸å­˜åœ¨")
            return
        
        # å¤„ç†æ¯ä¸ªmdæ–‡ä»¶
        md_files = list(doc_dir.glob("*.md"))
        print(f"ğŸ“š æ‰¾åˆ° {len(md_files)} ä¸ªæ–‡æ¡£æ–‡ä»¶")
        
        for file_path in md_files:
            try:
                print(f"\nğŸ“ å¤„ç†æ–‡æ¡£: {file_path.name}")
                
                # è¯»å–å†…å®¹
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                # æå–æ–‡æ¡£æ ‡é¢˜ï¼ˆå»æ‰å‰ç¼€ï¼‰
                document_title = file_path.name.replace('MinerU_markdown_', '').replace('.md', '')
                
                # ç®€å•çš„åˆ†å—ï¼šæŒ‰æ®µè½åˆ†å‰²
                paragraphs = [p.strip() for p in content.split('\n\n') if p.strip()]
                
                all_chunks = []
                for i, para in enumerate(paragraphs):
                    # è·³è¿‡å¤ªçŸ­çš„æ®µè½
                    if len(para) < 50:
                        continue
                    
                    # è®¡ç®—hash
                    text_hash = hashlib.sha256(para.encode()).hexdigest()
                    
                    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
                    from sqlalchemy import select
                    stmt = select(KnowledgeBaseChunk).where(
                        KnowledgeBaseChunk.chunk_text_hash == text_hash
                    )
                    result = await db.execute(stmt)
                    existing = result.scalar_one_or_none()
                    
                    if existing:
                        print(f"  è·³è¿‡é‡å¤å— {i}")
                        continue
                    
                    # åˆ›å»ºchunk
                    chunk = KnowledgeBaseChunk(
                        id=uuid.uuid4(),
                        source_type='disease_guideline',
                        disease_id=uuid.uuid4(),
                        disease_category='respiratory',
                        document_title=document_title,
                        section_title=f"æ®µè½_{i}",
                        chunk_index=i,
                        chunk_text=para,
                        chunk_text_hash=text_hash,
                        is_active=True,
                        created_at=datetime.utcnow()
                    )
                    
                    all_chunks.append(chunk)
                    
                    # æ¯10ä¸ªå—å¤„ç†ä¸€æ¬¡
                    if len(all_chunks) >= 10:
                        await process_chunks(all_chunks, vector_service, db)
                        all_chunks = []
                
                # å¤„ç†å‰©ä½™çš„å—
                if all_chunks:
                    await process_chunks(all_chunks, vector_service, db)
                
                print(f"âœ… æ–‡æ¡£å¤„ç†å®Œæˆ: {document_title}")
                
            except Exception as e:
                print(f"âŒ å¤„ç†å¤±è´¥ {file_path.name}: {e}")
                import traceback
                traceback.print_exc()
        
        print("\nğŸ‰ æ‰€æœ‰æ–‡æ¡£å¤„ç†å®Œæˆ")

async def process_chunks(chunks, vector_service, db):
    """å¤„ç†ä¸€æ‰¹chunks"""
    if not chunks:
        return
    
    # è¿‡æ»¤ç©ºæ–‡æœ¬
    valid_chunks = [c for c in chunks if c.chunk_text and c.chunk_text.strip()]
    if not valid_chunks:
        print("  æ²¡æœ‰æœ‰æ•ˆçš„æ–‡æœ¬å—")
        return
    
    texts = [c.chunk_text for c in valid_chunks]
    
    try:
        print(f"  ç”Ÿæˆ {len(texts)} ä¸ªå—çš„åµŒå…¥å‘é‡...")
        embeddings = await vector_service.generate_embeddings_batch(texts)
        
        # æ›´æ–°chunks
        config = await vector_service.get_active_config()
        for chunk, embedding in zip(valid_chunks, embeddings):
            chunk.embedding = embedding
            chunk.embedding_model_id = config.model_id if config else 'unknown'
            db.add(chunk)
        
        await db.commit()
        print(f"  âœ… å·²ä¿å­˜ {len(valid_chunks)} ä¸ªå—")
        
    except Exception as e:
        print(f"  âŒ ç”ŸæˆåµŒå…¥å¤±è´¥: {e}")
        await db.rollback()
        raise

if __name__ == "__main__":
    asyncio.run(vectorize_pediatric_asthma())
